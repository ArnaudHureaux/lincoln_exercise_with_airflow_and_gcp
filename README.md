# lincoln_exercise_with_airflow_and_gcp  
Exercise given by a consultant company to test my data engineering skills, including cloud storage and airflow features  
  
Prerequisites to execute this airflow DAG:  
Install airflow and launch it with Docker.  
Create a GCP service account key with the name "gcp_account_service_key.json"  
Create a GCP cloud storage named "servier_test_technique" with the datasources inside it:   
-datasource/pubmed.csv  
-datasource/pubmed.json  
-datasource/clinical_trials.csv  
-datasource/drugs.csv  
  
Install the pyton librairies: pip install -r requirements  

